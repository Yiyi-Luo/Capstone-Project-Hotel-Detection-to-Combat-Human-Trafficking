{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c19d95-cc48-4bf7-986f-ce626c178aa1",
   "metadata": {},
   "source": [
    "**Import various modules and packages for image processing, machine learning, and deep learning, including TensorFlow, Keras, scikit-learn, PyTorch, and Hugging Face Transformers, preparing for image classification and model training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5e253-d8f4-46cf-aa6c-72015dd91b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot graphviz\n",
    "sudo apt-get install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63b385-c241-481e-9a5d-d72118ca296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import copy2\n",
    "from collections import Counter\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Rescaling\n",
    "from tensorflow.keras import layers, regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "#For Cross Validation \n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#For basic Keras model training\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom, Dropout\n",
    "\n",
    "#For State-of-art pre-trained models\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import ResNet50, ResNet152V2, ResNet101\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB7, EfficientNetB6, EfficientNetV2S, EfficientNetV2M\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "#For ArcFace\n",
    "import math\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Input, Softmax\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3a44b6-a315-4c9d-8e51-f1c895a2bb39",
   "metadata": {},
   "source": [
    "**Check and remove corrupted images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69931762-f991-42b7-a5c4-fc2ec1f5909a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def delete_corrupted_images(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(root, filename)\n",
    "\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    img.verify()\n",
    "            except (IOError, SyntaxError) as e:\n",
    "                print(f\"PIL cannot process {file_path}, deleting: {e}\")\n",
    "                os.remove(file_path)\n",
    "                continue  \n",
    "\n",
    "            try:\n",
    "                image_bytes = tf.io.read_file(file_path)\n",
    "                image = tf.io.decode_image(image_bytes, channels=3, expand_animations=False)\n",
    "                image = tf.image.resize(image, [256, 256])\n",
    "            except tf.errors.InvalidArgumentError as e:\n",
    "                print(f\"TensorFlow cannot process {file_path}, deleting: {e}\")\n",
    "                os.remove(file_path)\n",
    "\n",
    "        for dirname in dirs:\n",
    "            dir_path = os.path.join(root, dirname)\n",
    "            if not os.listdir(dir_path):  \n",
    "                print(f\"Removing empty directory: {dir_path}\")\n",
    "                os.rmdir(dir_path)\n",
    "\n",
    "directory = 'train_images'\n",
    "delete_corrupted_images(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5601650-0534-4360-816d-7362b864a2f8",
   "metadata": {},
   "source": [
    "**Data Understanding and Overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d30e77-c6a0-444c-8257-7e4bd130f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'train_images'\n",
    "\n",
    "def analyze_images(dataset_path):\n",
    "    hotel_image_counts = {}\n",
    "\n",
    "    for hotel_id in os.listdir(dataset_path):\n",
    "        hotel_dir = os.path.join(dataset_path, hotel_id)\n",
    "        \n",
    "        if os.path.isdir(hotel_dir):\n",
    "            image_count = len([name for name in os.listdir(hotel_dir) if os.path.isfile(os.path.join(hotel_dir, name))])\n",
    "            hotel_image_counts[hotel_id] = image_count\n",
    "\n",
    "    total_hotels = len(hotel_image_counts)\n",
    "    min_images = min(hotel_image_counts.values())\n",
    "    max_images = max(hotel_image_counts.values())\n",
    "    avg_images = sum(hotel_image_counts.values()) / total_hotels\n",
    "\n",
    "    image_count_frequency = Counter(hotel_image_counts.values())\n",
    "\n",
    "    top_10_common_counts = image_count_frequency.most_common(10)\n",
    "    counts = [count for count, num_hotels in top_10_common_counts]\n",
    "    hotels = [num_hotels for count, num_hotels in top_10_common_counts]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(counts)), hotels, tick_label=counts)\n",
    "    plt.xlabel('Number of Images per Hotel')\n",
    "    plt.ylabel('Number of Hotels')\n",
    "    plt.title('Top 10 Most Common Image Counts')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    print(f\"Total number of hotels: {total_hotels}\")\n",
    "    print(f\"Minimum number of images in a hotel: {min_images}\")\n",
    "    print(f\"Maximum number of images in a hotel: {max_images}\")\n",
    "    print(f\"Average number of images per hotel: {avg_images:.2f}\")\n",
    "    print(\"The top 10 most common image counts are:\")\n",
    "    for count, num_hotels in top_10_common_counts:\n",
    "        print(f\"{num_hotels} hotels have {count} images\")\n",
    "\n",
    "analyze_images(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcf4e6d-4e9c-4b6c-921d-aba41bebb7c0",
   "metadata": {},
   "source": [
    "**Challenge 1 - Class Diversity:** The dataset comprises a **large number of classes (1,678 distinct hotels)**, which presents a substantial challenge in terms of learning distinct features for each class.\n",
    "\n",
    "**Challenge 2 - Class Imbalance:** There is **significant imbalance** in the dataset, with some hotels represented by a single image while others have over a thousand. This disparity can lead to overfitting and poor generalization for underrepresented classes.\n",
    "\n",
    "**Challenge 3 - Insufficient Learning Samples:** Most hotels have **fewer than 15 images** available, which may not provide enough data for the model to effectively learn the variance within each class.\n",
    "\n",
    "**Challenge 4 - Subtle Variations:** Hotel rooms often share similar aesthetics and structures, much like nuances in human faces, making it **difficult to distinguish between different hotels**.\n",
    "\n",
    "**Challenge 5 - Occlusions from Privacy Masks:** The presence of **occlusion masks** (red masks) adds complexity to the task, as they can **cover significant portions** of the images, obscuring important details that are necessary for accurate classification.\n",
    "\n",
    "**Challenge 6 - Potential Overfitting:** With a limited number of images per class, there is a risk of models overfitting to the training data, which could result in poor performance on unseen data.\n",
    "\n",
    "**Challenge 7 - Background Noise and Variability:** The variability in lighting, decor, and camera angles within hotel room images introduces additional complexity, potentially leading to background noise that can confuse the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aceb93-20a5-4aee-8b99-38ce9ba4bace",
   "metadata": {},
   "source": [
    "**The process_images function resizes images, applies random red masks to them, and saves the processed images to a specified directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4561497a-a4c5-4ba7-98b2-29a1bc60a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to resize images\n",
    "def resize_image (image_path, target_size=(256,256)):\n",
    "    \"\"\"Resize an image to the target size.\"\"\"\n",
    "    image = load_img(image_path)\n",
    "    image = image.resize(target_size)\n",
    "    return image\n",
    "\n",
    "#A funtion to apply masks\n",
    "def apply_mask (image, mask):\n",
    "    \"\"\"Apply a red mask to an image.\"\"\"\n",
    "    image_array = img_to_array (image)\n",
    "    mask_array = img_to_array (mask)\n",
    "    #Maks's red color is the pure red color [255,0,0]\n",
    "    red_mask = (mask_array[:,:,0] == 255) & (mask_array[:,:,1] == 0) & (mask_array[:,:,2] == 0)\n",
    "    #Apply the red mask to each channel of the image\n",
    "    image_array[red_mask] = [255,0,0] #This sets the color to red wherever the mask is present\n",
    "    return array_to_img(image_array)\n",
    "\n",
    "#A function to process the images by resizing and applying masks\n",
    "def process_images(train_images_dir, train_masks_dir, output_dir, target_size=(256, 256)):\n",
    "    \"\"\"Process images by resizing and applying masks.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Initialize a counter for processed images\n",
    "    processed_count = 0\n",
    "\n",
    "    # List all subdirectories in train_images_dir\n",
    "    subdirs = [d for d in os.listdir(train_images_dir) if os.path.isdir(os.path.join(train_images_dir, d))]\n",
    "    \n",
    "    mask_files = [f for f in os.listdir(train_masks_dir) if f.lower().endswith('.png')]\n",
    "\n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(train_images_dir, subdir)\n",
    "        image_files = [f for f in os.listdir(subdir_path) if f.lower().endswith('.jpg')]\n",
    "\n",
    "        for image_file in image_files:\n",
    "            try:\n",
    "                image_path = os.path.join(subdir_path, image_file)\n",
    "                mask_file = random.choice(mask_files)  # Random mask for each image\n",
    "                mask_path = os.path.join(train_masks_dir, mask_file)\n",
    "\n",
    "                resized_image = resize_image(image_path, target_size)\n",
    "                resized_mask = resize_image(mask_path, target_size)\n",
    "\n",
    "                masked_image = apply_mask(resized_image, resized_mask)\n",
    "\n",
    "                output_path = os.path.join(output_dir, f\"{subdir}_{image_file}\")\n",
    "                masked_image.save(output_path)\n",
    "                processed_count += 1\n",
    "            except Exception as e:\n",
    "                print (f\"Error processing image {image_path}: {e}\")\n",
    "\n",
    "    print(f\"Processed {processed_count} images and saved to {output_dir}\")\n",
    "\n",
    "def copy_files(files, source_dir, dest_dir):\n",
    "    for f in files:\n",
    "        shutil.copy(os.path.join(source_dir, f), os.path.join(dest_dir, f))\n",
    "\n",
    "train_images_dir = 'train_images'\n",
    "train_masks_dir = 'train_masks'\n",
    "processed_images_dir ='processed_images'\n",
    "\n",
    "if os.path.exists(processed_images_dir):\n",
    "    shutil.rmtree(processed_images_dir)\n",
    "os.makedirs(processed_images_dir)\n",
    "\n",
    "process_images(train_images_dir, train_masks_dir, processed_images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feb10f2-7a24-4e8f-a39a-d444ee2324f5",
   "metadata": {},
   "source": [
    "**Create training and validation datasets from the 'train_images' directory, using TensorFlow to preprocess images into batches of 48 with a size of 256x256 pixels; Split the data into 70% for training and 30% for validation and testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f3b2a-04cb-4e46-821e-a92d2a9661ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'processed_images'\n",
    "\n",
    "full_train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=directory,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=48,\n",
    "    image_size=(256,256),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.3, \n",
    "    subset='training', \n",
    "    interpolation='bilinear', \n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False\n",
    ")\n",
    "\n",
    "full_validation_test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=directory,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    color_mode='rgb',\n",
    "    batch_size=48,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.3,  \n",
    "    subset='validation',  \n",
    "    interpolation='bilinear',\n",
    "    crop_to_aspect_ratio=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425129fb-ed82-4622-a643-4e087a795cc5",
   "metadata": {},
   "source": [
    "**Calculate the total number of batches in a validation and test dataset, split it evenly into two subsets for validation and testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a41193-93f5-4752-b41c-35348cffa648",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batches = len(full_validation_test_dataset)\n",
    "batches_per_subset = total_batches // 2\n",
    "\n",
    "full_validation_dataset = full_validation_test_dataset.take(batches_per_subset)\n",
    "\n",
    "full_test_dataset = full_validation_test_dataset.skip(batches_per_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d50c2e-a4ca-4082-9292-83c097c2d1a5",
   "metadata": {},
   "source": [
    "**Define a data augmentation pipeline using Keras, incorporating random flips, rotations, zooms, contrast and brightness adjustments, resizing to 280x280 pixels, cropping back to 256x256 pixels, and translations to enhance the diversity of the training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91192224-d8e0-4aed-a44f-c82fa511ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal_and_vertical\", input_shape=(256, 256, 3)),  \n",
    "        layers.RandomRotation(0.2),  \n",
    "        layers.RandomZoom(0.2),  \n",
    "        layers.RandomContrast(factor=0.1),  \n",
    "        layers.RandomBrightness(factor=0.1),  \n",
    "        layers.Resizing(280, 280),  \n",
    "        layers.RandomCrop(height=256, width=256),  \n",
    "        layers.RandomTranslation(height_factor=0.1, width_factor=0.1), \n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068befac-78f2-4ff7-aa1e-5ddc712aa2a8",
   "metadata": {},
   "source": [
    "**Part II: Basic Keras Models**\n",
    "\n",
    "**Build a machine learning workflow for image classification using TensorFlow and Keras:**\n",
    "\n",
    "**1.** **Set up essential parameters**: class names, the number of classes, and the number of epochs for training. The dataset is prepared using the ImageDataGenerator for image rescaling, ensuring proper input to the neural network;\n",
    "   \n",
    "**2.** A versatile model creation function is defined to **construct CNN models with optional data augmentation and dropout**, making it adaptable to various scenarios. The compile_and_train_model function takes care of compiling and training the models;\n",
    "\n",
    "**3.** **Visualize training results** by the plot_training_history function, providing plots of training and validation accuracy and loss, aiding in the interpretation of the model's performance over epochs.\n",
    "   \n",
    "**4.** **Implements K-Fold cross-validation** to ensure the model's robustness and generalizability. It generates training and validation data generators for each fold, trains the model, and computes the average accuracy, offering a comprehensive evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc5067-3b46-4d9c-87fd-47b2d1d8199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = full_train_dataset.class_names\n",
    "num_classes = len(class_names)\n",
    "epochs = 10\n",
    "directory = 'full_train_dataset'\n",
    "n_splits = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87447c52-9298-445f-96b9-2dd9ba757aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes, use_data_augmentation=False, dropout_rate=None):\n",
    "    layers_list = []\n",
    "    if use_data_augmentation:\n",
    "        layers_list.append(data_augmentation)\n",
    "    layers_list.extend([\n",
    "        layers.Rescaling(1./255, input_shape=(256, 256, 3)),\n",
    "        layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "    ])\n",
    "    if dropout_rate is not None:\n",
    "        layers_list.append(layers.Dropout(dropout_rate))\n",
    "    layers_list.extend([\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return Sequential(layers_list)\n",
    "\n",
    "def compile_and_train_model(model, train_ds, val_ds, epochs):\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "def plot_training_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs_range = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show\n",
    "    \n",
    "\n",
    "def cross_validate_model(create_model_func, model_params, epochs, n_splits):\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=123)\n",
    "\n",
    "    fold_no = 1\n",
    "    scores = []\n",
    "\n",
    "    for train_index, val_index in kfold.split(np.arange(total_batches)):\n",
    "        print(f'Fold {fold_no} --')\n",
    "\n",
    "        train_subset = tf.data.experimental.cardinality(full_train_dataset).numpy()\n",
    "        val_subset = tf.data.experimental.cardinality(full_validation_dataset).numpy()\n",
    "\n",
    "        train_batches = int(train_subset * len(train_index) / total_batches)\n",
    "        val_batches = int(val_subset * len(val_index) / total_batches)\n",
    "\n",
    "        train_ds = full_train_dataset.take(train_batches)\n",
    "        val_ds = full_validation_dataset.skip(val_batches).take(val_batches)\n",
    "\n",
    "        model = create_model_func(**model_params)\n",
    "        compile_and_train_model(model, train_ds, val_ds, epochs)\n",
    "\n",
    "        scores.append(model.evaluate(val_ds))\n",
    "\n",
    "        fold_no += 1\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    print('Scores for each fold:', scores)\n",
    "    print('Average score:', np.mean(scores, axis=0))\n",
    "    print('Standard deviation:', np.std(scores, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4cfa85-48a9-4b47-8ef1-4f29d7ed0988",
   "metadata": {},
   "source": [
    "**A basic Keras convolutional neural network model is created without any data augmentation or dropout features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e35ba-39ae-4e7c-b21e-4bb6d8ae0682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Basic Keras model without data augmentation and dropout\n",
    "model_basic = create_model(num_classes)\n",
    "history_basic = compile_and_train_model(model_basic, full_train_dataset, full_validation_dataset, epochs)\n",
    "plot_training_history(history_basic)\n",
    "\n",
    "model_basic_config = {\"func\": create_model, \"params\": {\"num_classes\": num_classes}}\n",
    "print(\"Running cross-validation for the basic Keras model\")\n",
    "average_accuracy_basic = cross_validate_model(model_basic_config['func'], model_basic_config['params'], epochs, n_splits)\n",
    "print(f\"Average accuracy for basic model: {average_accuracy_basic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07be5534-c7a6-4898-91b1-79e263f3159e",
   "metadata": {},
   "source": [
    "**Incorporating data augmentation to the Keras convolutional neural network model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81252b39-9cee-4bfb-9010-051dd36ad646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basic Keras Model with data augmentation\n",
    "model_with_aug = create_model(num_classes, use_data_augmentation=True)\n",
    "history_with_aug = compile_and_train_model(model_with_aug, full_train_dataset, full_validation_dataset, epochs)\n",
    "plot_training_history(history_with_aug)\n",
    "\n",
    "model_with_augmentation_config = {\"func\": create_model, \"params\": {\"num_classes\": num_classes, \"use_data_augmentation\": True}}\n",
    "print(\"\\nRunning cross-validation for the basic Keras model with augmentation\")\n",
    "average_accuracy_augmentation = cross_validate_model(model_with_augmentation_config['func'], model_with_augmentation_config['params'], epochs, n_splits)\n",
    "print(f\"Average accuracy for model with augmentation: {average_accuracy_augmentation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b48d7ce-d808-45b3-9e98-6f1e0b26804b",
   "metadata": {},
   "source": [
    "**Incorporating data augmentation and dropout to the Keras convolutional neural network model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e1e3c-6b24-4148-9196-c679af4cc4fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basic Keras Model with data augmentation and dropout\n",
    "model_dropout_aug = create_model(num_classes, use_data_augmentation=True, dropout_rate=0.2)\n",
    "history_dropout_aug = compile_and_train_model(model_dropout_aug, full_train_dataset, full_validation_dataset, epochs)\n",
    "plot_training_history(history_dropout_aug)\n",
    "\n",
    "model_with_augmen_dropout_config = {\"func\": create_model, \"params\": {\"num_classes\": num_classes, \"use_data_augmentation\": True, \"dropout_rate\": 0.2}}\n",
    "print(\"\\nRunning cross-validation for the basic Keras model with augmentation and dropout\")\n",
    "average_accuracy_dropout = cross_validate_model(model_with_augmen_dropout_config['func'], model_with_augmen_dropout_config['params'], epochs, n_splits)\n",
    "print(f\"Average accuracy for model with augmentation and dropout: {average_accuracy_dropout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea89f11-27dd-4a90-96d6-02ad7f3c5a66",
   "metadata": {},
   "source": [
    "**Part III: More advanced and pre-trained convolutional neural network models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8544ad-4bb7-461a-8152-1caf050a337a",
   "metadata": {},
   "source": [
    "**The limitations of a basic Keras model** in handling such a complex and nuanced task stem from its typically shallow architecture, which might lack the capacity to capture the subtle differences between highly similar hotel rooms. \n",
    "\n",
    "While **VGG and Inception**, the deeper convolutional neural networks, offer improvements by delving deeper into the image structure through its multiple layers.\n",
    "\n",
    "Given the extensive size of our dataset, comprising **22,244 images across 1,674 classes**, coupled with the constraints of **limited computational resources**, we opted for a strategic approach. We **selected a smaller subset of the dataset (the top 500 classes with the most images)** to conduct preliminary tests on various deeper CNN pre-trained models. By concentrating on **tweaking the top-performing models**, we plan to gradually make them even better. This careful way of doing things helps us deal with our limited resources and still get the best results we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665fca7a-2548-490e-a614-262eda59a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_directory = \"processed_images\"\n",
    "\n",
    "image_counts = {}\n",
    "\n",
    "for class_dir in os.listdir(full_dataset_directory):\n",
    "    class_path = os.path.join(full_dataset_directory, class_dir)\n",
    "    if os.path.isdir(class_path):\n",
    "        image_counts[class_dir] = len([entry for entry in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, entry))])\n",
    "\n",
    "top_500_classes = Counter(image_counts).most_common(500)\n",
    "\n",
    "subset_directory = \"subset_top_500_hotels\"  \n",
    "os.makedirs(subset_directory, exist_ok=True)\n",
    "\n",
    "for class_dir, _ in top_500_classes:\n",
    "    original_class_path = os.path.join(full_dataset_directory, class_dir)\n",
    "    subset_class_path = os.path.join(subset_directory, class_dir)\n",
    "    os.makedirs(subset_class_path, exist_ok=True)\n",
    "    \n",
    "    for image in os.listdir(original_class_path):\n",
    "        src_image_path = os.path.join(original_class_path, image)\n",
    "        dst_image_path = os.path.join(subset_class_path, image)\n",
    "        copy2(src_image_path, dst_image_path)\n",
    "\n",
    "print(\"The subset with the top 500 classes has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a336c44-efe3-4c07-b169-ec2d901795b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'subset_top_500_hotels'\n",
    "analyze_images(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385e4eb-a7c6-47f4-9b0f-196defc6396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_directory = 'subset_top_500_hotels'\n",
    "delete_corrupted_images(subset_directory)\n",
    "\n",
    "subset_train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=subset_directory,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=48,\n",
    "    image_size=(256,256),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.3, \n",
    "    subset='training', \n",
    "    interpolation='bilinear', \n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False\n",
    ")\n",
    "\n",
    "subset_validation_test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=subset_directory,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    color_mode='rgb',\n",
    "    batch_size=48,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.3,  \n",
    "    subset='validation',  \n",
    "    interpolation='bilinear',\n",
    "    crop_to_aspect_ratio=False\n",
    ")\n",
    "\n",
    "total_batches = len(subset_validation_test_dataset)\n",
    "batches_per_subset = total_batches // 2\n",
    "subset_validation_dataset = subset_validation_test_dataset.take(batches_per_subset)\n",
    "subset_test_dataset = subset_validation_test_dataset.skip(batches_per_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a364118b-d06e-4951-99df-b6f779d271c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model_name, num_classes, input_shape=(256, 256, 3), learning_rate=0.001):\n",
    "    if model_name == 'VGG16':\n",
    "        base_model = VGG16(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    elif model_name == 'InceptionV3':\n",
    "        base_model = InceptionV3(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    elif model_name == 'ResNet152V2':\n",
    "        base_model = ResNet152V2(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    elif model_name == 'ResNet101':\n",
    "        base_model = ResNet101(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    elif model_name == 'ResNet50':\n",
    "        base_model = ResNet50(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    elif model_name == 'DenseNet121':\n",
    "        base_model = DenseNet121(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    elif model_name == 'EfficientNetB7':\n",
    "        base_model = EfficientNetB7(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    elif model_name == 'EfficientNetB6':\n",
    "        base_model = EfficientNetB6(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    elif model_name == 'EfficientNetB0':\n",
    "        base_model = EfficientNetB0(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    elif model_name == 'EfficientNetV2S':\n",
    "        base_model = EfficientNetV2S(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    elif model_name == 'EfficientNetV2M':\n",
    "        base_model = EfficientNetV2M(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)  \n",
    "    x = Dense(1024, activation='relu')(x)  \n",
    "    output = Dense(num_classes, activation='softmax')(x)  \n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_dataset, validation_dataset, epochs=10):\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=10,\n",
    "        verbose=1\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e28219-fea0-45ef-9aef-be4a4107349b",
   "metadata": {},
   "source": [
    "**VGG-16:** Progress from a basic Keras model to the more advanced and pre-trained VGG-16 model for image classification. **VGG-16 is a powerful convolutional neural network pre-trained on the ImageNet dataset**, we are hoping to leverage deeper and more complex architectures that have been proven effective on a wide range of image recognition tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845d3e7-800f-428a-b72e-20169f985d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = subset_train_dataset.class_names\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e932609-4f05-4cc0-9c3c-0af857c2efd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'VGG16'  \n",
    "model_VGG16 = prepare_model(model_name, num_classes)\n",
    "history = train_model(model_VGG16, subset_train_dataset, subset_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb936f6-08e2-4ba6-afaf-b59ff9433365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'InceptionV3'  \n",
    "model_InceptionV3 = prepare_model(model_name, num_classes)\n",
    "history = train_model(model_InceptionV3, subset_train_dataset, subset_validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2afd9c-8ffd-4838-ad8e-b5ebdabfe32d",
   "metadata": {},
   "source": [
    "**VGG and Inception** represent earlier convolutional neural network (CNN) designs focusing on **depth** and **multi-scale** processing, while models like **ResNet, DenseNet, and EfficientNet** are more advanced iterations that incorporate additional mechanisms to handle the challenges of deeper network structures. \n",
    "\n",
    "**ResNet** introduces a novel approach to facilitate training deeper networks through skip connections;\n",
    "\n",
    "**DenseNet** streamlines the training of deep architectures by densely connecting each layer to every other layer, ensuring maximum information flow between layers. This design not only enhances performance with a more efficient parameter usage but also helps alleviate the vanishing-gradient problem, making deep networks easier to train;\n",
    "\n",
    "**EfficientNet** optimizes CNN scaling by uniformly increasing depth, width, and resolution with fixed coefficients, leading to state-of-the-art performance on image classification tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad28acb6-a3fe-410d-8092-3942283324ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'ResNet152V2'  \n",
    "model_ResNet152V2 = prepare_model(model_name, num_classes)\n",
    "history = train_model(model_ResNet152V2, subset_train_dataset, subset_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969fe146-3281-48aa-9df2-65f082839328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'ResNet101'  \n",
    "model_ResNet101 = prepare_model(model_name, num_classes)\n",
    "history = train_model(model_ResNet101, subset_train_dataset, subset_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8b9397-4ce4-47d4-b042-866e34cfe2d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'ResNet50'  \n",
    "model_ResNet50 = prepare_model(model_name, num_classes)\n",
    "history = train_model(model_ResNet50, subset_train_dataset, subset_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d32bc0-47fd-42c1-896f-4571aa4424dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'DenseNet121'  \n",
    "model_DenseNet121 = prepare_model(model_name, num_classes)\n",
    "history = train_model(model_DenseNet121, subset_train_dataset, subset_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f15cdf-4c0f-467e-b356-67cda19e3369",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'EfficientNetB7'  \n",
    "model_EfficientNetB7 = prepare_model(model_name, num_classes)\n",
    "history = train_model(model_EfficientNetB7, subset_train_dataset, subset_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111cb372-ba15-4b29-9d68-3c75b2fae314",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'EfficientNetB6'  \n",
    "model_EfficientNetB6 = prepare_model(model_name, num_classes)\n",
    "history = train_model(model_EfficientNetB6, subset_train_dataset, subset_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e8f740-87a0-4c6b-9dca-cfd345541e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'EfficientNetB0'  \n",
    "model_EfficientNetB0 = prepare_model(model_name, num_classes)\n",
    "history = train_model(model_EfficientNetB0, subset_train_dataset, subset_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f4f60e-a7d9-4674-8639-efc51a4ae230",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'EfficientNetV2S'  \n",
    "model_EfficientNetV2S = prepare_model(model_name, num_classes)\n",
    "history = train_model(model_EfficientNetV2S, subset_train_dataset, subset_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce3e867-c439-4cbc-94d9-35528a1c831e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'EfficientNetV2M'  \n",
    "model_EfficientNetV2M = prepare_model(model_name, num_classes)\n",
    "history = train_model(model_EfficientNetV2M, subset_train_dataset, subset_validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef3aedf-903b-4885-b13a-edd24dec0f2a",
   "metadata": {},
   "source": [
    "**Among all the pre-trained models, EfficientNetB0 stood out; we continue to use the EfficientNetB0 architecture as a base, leveraging transfer learning, regularization, and data augmentation to enhance performance on potentially complex datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad5a9f-69d5-4254-996d-9ec6ff5261aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes, input_shape=(256, 256, 3), l1_factor=0, l2_factor=0, dropout_rate=0, use_data_augmentation=False):\n",
    "    base_model = EfficientNetB0(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    if use_data_augmentation:\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "            layers.RandomFlip(\"horizontal_and_vertical\"),  \n",
    "            layers.RandomRotation(0.2),  \n",
    "            layers.RandomZoom(0.2),  \n",
    "            layers.RandomContrast(factor=0.1),  \n",
    "            layers.RandomBrightness(factor=0.1),  \n",
    "            layers.Resizing(280, 280),  \n",
    "            layers.RandomCrop(height=256, width=256),  \n",
    "            layers.RandomTranslation(height_factor=0.1, width_factor=0.1), \n",
    "        ])\n",
    "        x = data_augmentation(x)\n",
    "    \n",
    "    x = base_model(x, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    if dropout_rate > 0:\n",
    "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=l1_factor, l2=l2_factor))(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l1_l2(l1=l1_factor, l2=l2_factor))(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def compile_and_fit(model, train_dataset, validation_dataset, epochs=50):\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507058f5-462a-4b3f-8c51-1d6d4fe119b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EfficientNetB0 Model without augmentation, drop out or regularization\n",
    "Efficient_base_model = build_model(num_classes)\n",
    "history = compile_and_fit(Efficient_base_model, subset_train_dataset, subset_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ef40c-746d-468a-8510-9d49271fbfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EfficientNetB0 Model with l1+l2 Regularization, dropout and augmentation\n",
    "model_with_reg = build_model(num_classes, l1_factor=1e-5, l2_factor=1e-4, dropout_rate=0.2, use_data_augmentation=True)\n",
    "history_with_reg = compile_and_fit(model_with_reg, subset_train_dataset, subset_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd03e5-c259-44ae-89db-017b44f4f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EfficientNetB0 Model with l1+l2 Regularization\n",
    "model_with_reg = build_model(num_classes, l1_factor=1e-5, l2_factor=1e-4)\n",
    "history_with_reg = compile_and_fit(model_with_reg, subset_train_dataset, subset_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb0242b-9ed7-43e3-aad6-28f393b3500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EfficientNetB0 Model with dropout (0.2)\n",
    "model_with_reg = build_model(num_classes, dropout_rate=0.2)\n",
    "history_with_reg = compile_and_fit(model_with_reg, subset_train_dataset, subset_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c764a81-4fc9-4ffc-90fd-ec716bf43fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EfficientNetB0 Model with augmentation\n",
    "model_with_reg = build_model(num_classes, use_data_augmentation=True)\n",
    "history_with_reg = compile_and_fit(model_with_reg, subset_train_dataset, subset_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d22fd2d-e57d-4b83-92d0-0f517e491e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EfficientNetB0 Model with l1+l2 Regularization and dropout \n",
    "model_with_reg = build_model(num_classes, l1_factor=1e-5, l2_factor=1e-4, dropout_rate=0.2)\n",
    "history_with_reg = compile_and_fit(model_with_reg, subset_train_dataset, subset_validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c7056-9027-4f7d-878b-0a948dbd7f28",
   "metadata": {},
   "source": [
    "### **ArcFace Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e99ef-0da4-4952-b12f-4c4d9a879a53",
   "metadata": {},
   "source": [
    "To address the challenges akin to those in facial recognition, incorporating **ArcFace** into our model presents a compelling solution. The ArcMarginProduct class, inspired by the ArcFace method, is a custom TensorFlow layer designed to **significantly enhance the discriminative power of feature embeddings** in deep learning models, particularly beneficial for classification tasks where subtle differences between classes, such as facial features or hotel rooms, are paramount. ArcFace stands out for its innovative approach of enforcing an angular margin between classes in the feature space. This technique effectively amplifies the model's **sensitivity to intra-class variations** and ensures a clearer separation between different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023cec5-dad4-43f7-ac97-879068fdde88",
   "metadata": {},
   "source": [
    "<img width=\"1300\" alt=\"Screenshot 2024-02-10 at 5 47 44 PM\" src=\"https://github.com/Yiyi-Luo/Capstone-Project-Hotel-Detection-to-Combat-Human-Trafficking/assets/149438809/347415ed-0fea-43fa-b38f-6f777098c0f7\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c5106-5866-4ebb-b416-33bdbd35363e",
   "metadata": {},
   "source": [
    "By adjusting the cosine distance between feature vectors and class centers with a **scale (s) and margin (m)** parameter, ArcFace makes the model more adept at distinguishing between closely related categories. The ArcMarginProduct layer implements this by applying the angular margin in the cosine space, offering options for easy margin settings and **label smoothing (ls_eps)** to promote training stability and enhance generalization. This layer is particularly useful in scenarios demanding high discriminative capabilities, such as identifying nuanced differences in hotel room categories or in facial recognition tasks, ensuring the model learns robust and distinct features for each class, thereby improving accuracy and generalization in scenarios characterized by subtle variances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e886e45-5543-4912-ae4c-9d3556bf3e50",
   "metadata": {},
   "source": [
    "<img width=\"1176\" alt=\"Screenshot 2024-02-10 at 5 48 22 PM\" src=\"https://github.com/Yiyi-Luo/Capstone-Project-Hotel-Detection-to-Combat-Human-Trafficking/assets/149438809/c4559c51-fc88-44e3-9126-676c95ded963\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8d9991-10da-4fb5-8fbf-d79cc0809590",
   "metadata": {},
   "source": [
    "#### **References:**\n",
    "##### 1. https://arxiv.org/pdf/1801.07698.pdf (**Authors:** Jiankang Deng, Jia Guo, Jing Yang, Niannan Xue, Irene Kotsia, and Stefanos Zafeiriou)\n",
    "##### 2. [How to train with ArcFace loss to improve model classification accuracy | by Yiwen Lai](https://yiwenlai.medium.com/how-to-train-with-arcface-loss-to-improve-model-classification-accuracy-d4035195aeb9) (**Author:** Yiwen Lai)\n",
    "##### 3. https://github.com/Niellai/ObjectDetection/blob/master/10_COVID19_ArcFace.ipynb?source=post_page-----d4035195aeb9-------------------------------- (**Author:** Niel Lai)\n",
    "##### 4. https://www.kaggle.com/code/hidehisaarai1213/glret21-efficientnetb0-baseline-inference/notebook (**Author:** Hidehisa Arai)\n",
    "##### 5. https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/blob/master/src/modeling/metric_learning.py (**Author:** Lyakaap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f432ea-2383-4c4d-b720-c1a9e5d77a5b",
   "metadata": {},
   "source": [
    "**Preparing dataset for ArcFace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5737c029-0cfc-48cc-995c-949b228d1519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_arcface(images, labels):\n",
    "    return (images, labels), labels\n",
    "\n",
    "subset_train_dataset = subset_train_dataset.map(prepare_for_arcface)\n",
    "subset_validation_dataset = subset_validation_dataset.map(prepare_for_arcface)\n",
    "subset_test_dataset = subset_test_dataset.map(prepare_for_arcface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9511f505-a6f3-41d8-9d14-6d23c6230aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcMarginProduct(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Implements large margin arc distance.\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/pdf/1801.07698.pdf (Authors: Jiankang Deng, Jia Guo, Jing Yang, Niannan Xue, Irene Kotsia, and Stefanos Zafeiriou)\n",
    "        https://www.kaggle.com/code/hidehisaarai1213/glret21-efficientnetb0-baseline-inference/notebook (Author: Hidehisa Arai)\n",
    "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/ \n",
    "            blob/master/src/modeling/metric_learning.py (Author: Lyakaap)\n",
    "    '''\n",
    "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
    "                 ls_eps=0.0, **kwargs):\n",
    "\n",
    "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = tf.math.cos(m)\n",
    "        self.sin_m = tf.math.sin(m)\n",
    "        self.th = tf.math.cos(math.pi - m)\n",
    "        self.mm = tf.math.sin(math.pi - m) * m\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_classes': self.n_classes,\n",
    "            's': self.s,\n",
    "            'm': self.m,\n",
    "            'ls_eps': self.ls_eps,\n",
    "            'easy_margin': self.easy_margin,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcMarginProduct, self).build(input_shape[0])\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            dtype='float32',\n",
    "            trainable=True,\n",
    "            regularizer=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X, y = inputs\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        cosine = tf.matmul(\n",
    "            tf.math.l2_normalize(X, axis=1),\n",
    "            tf.math.l2_normalize(self.W, axis=0)\n",
    "        )\n",
    "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = tf.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        one_hot = tf.cast(\n",
    "            tf.one_hot(y, depth=self.n_classes),\n",
    "            dtype=cosine.dtype\n",
    "        )\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5bead-767f-4bdf-9cc9-8d74e686314d",
   "metadata": {},
   "source": [
    "**We freeze all but the last three layers of EfficientNetB0 for feature extraction, apply global average pooling, and optionally include dropout for regularization. The ArcMarginProduct layer, which requires both feature and label inputs, is utilized to enforce an angular margin that enhances the separability between classes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef9c15-7dd4-4dd5-b9e9-360690f12b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_with_arcface(num_classes, input_shape=(256, 256, 3), dropout_rate=0, l1_factor=0, l2_factor=0):\n",
    "    '''\n",
    "    Build model using EfficientNetB0 as base and ArcFace as output layer.\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/pdf/1801.07698.pdf(Authors: Jiankang Deng, Jia Guo, Jing Yang, Niannan Xue, Irene Kotsia, and Stefanos Zafeiriou)\n",
    "        How to train with ArcFace loss to improve model classification accuracy | by Yiwen Lai (Author: Yiwen Lai)\n",
    "        https://github.com/Niellai/ObjectDetection/blob/master/10_COVID19_ArcFace.ipynb?source=post_page-----d4035195aeb9--------------------------------(Author: Niel Lai)\n",
    "        https://www.kaggle.com/code/hidehisaarai1213/glret21-efficientnetb0-baseline-inference/notebook (Author: Hidehisa Arai)\n",
    "\n",
    "    '''\n",
    "    premodel = EfficientNetB0(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    \n",
    "    # Freeze the layers except the last 3\n",
    "    for layer in premodel.layers[:-3]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Define the input layers\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    label_input = Input(shape=(num_classes,))  # Additional input for labels\n",
    "    \n",
    "    # Forward pass through the base model\n",
    "    x = premodel(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    if dropout_rate > 0:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=l1_factor, l2=l2_factor))(x)\n",
    "    \n",
    "    output = ArcMarginProduct(n_classes=num_classes, s=30.0, m=0.5)([x, label_input])\n",
    "    \n",
    "    model = Model([inputs, label_input], output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compile_and_fit(model, train_dataset, validation_dataset, epochs=30, callbacks=None):\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-5),  \n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,  \n",
    "        verbose=1,\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f95cbd9-3941-4974-b67a-9e6f980f836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'model_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "model_path = os.path.join(checkpoint_dir, 'best_model.h5')\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, mode='min')\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(model_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, mode='min'),\n",
    "]\n",
    "\n",
    "input_shape = (256, 256, 3)\n",
    "num_classes = 500  \n",
    "dropout_rate = 0.2\n",
    "l1_factor = 1e-4\n",
    "l2_factor = 1e-4\n",
    "model = build_model_with_arcface(num_classes, input_shape, l1_factor, l2_factor, dropout_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdfa855-0486-41c6-ab7f-6401f9adb427",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model_architecture.png', show_shapes=True)\n",
    "Image(filename='model_architecture.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b486bf-89a6-446f-9bbe-97008301ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = compile_and_fit(model, subset_train_dataset, subset_validation_dataset, epochs=30, callbacks=callbacks)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(subset_test_dataset)\n",
    "print(f'Test accuracy: {test_accuracy}, Test loss: {test_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
